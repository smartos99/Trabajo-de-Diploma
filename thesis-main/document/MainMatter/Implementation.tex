\chapter{Detalles de Implementación y Experimentos}\label{chapter:implementation}

\section{Instalación de la biblioteca Coqui}
Coqui [\cite{coqui-doc}] es un repositorio de código abierto que implementa las últimas investigaciones en materia de síntesis de voz, como Tacotron 2 y VITS que son los modelos base utilizados en el presente proyecto. Este repositorio ha sido usado para generar modelos en más de 20 idiomas  y cuenta además con múltiples ``recetas'' para el entrenamiento de modelos. 

La biblioteca se instala de acuerdo a las instrucciones orientadas a desarrolladores en [\cite{coqui-doc}].
Con esto ya es suficiente para probar los modelos preentrenados disponibles de Coqui.


\section{Creación de base de datos con voces cubanas.}

\subsubsection{¿Qué hace a un buen Dataset?}

\begin{itemize}
	\item Debe cubrir una cantidad considerable de clips cortos y largos.
	\item Libre de errores. Se debe eliminar cualquier archivo incorrecto o roto. 
	\item Para escuchar una voz con la mayor naturalidad posible con todas las diferencias de frecuencia y tono, por ejemplo, usando diferentes signos de puntuación.
	\item Es necesario que el \textit{dataset} cubra una buena parte de fonemas, difonemas y, en algunos idiomas, trifonemas. Si la cobertura de fonemas es baja, el modelo puede tener dificultades para pronunciar nuevas palabras difíciles.
	\item Las muestras de la base de datos deben estar lo más limpio posible, es decir, se debe limpiar de ruido y cortar los espacios de tiempo entre expresiones,donde no se hable.
	
\end{itemize}


\subsubsection{Cuban Voice Dataset}
La base de datos está conformada por 160 clips de audio con sus respectivas transcripciones recogidas en el archivo metadata.csv. Cada clip tiene una duración de 2 a 15 segundos, no más, para evitar sobrecargar los métodos que tienen que ver con la alineación.\\

Los clips de audio poseen formato .wav y se organizan dentro de una carpeta de nombre \textit{wavs} de la siguiente forma:

\begin{center}
	/wavs\\
	| - audio1.wav\\
	| - audio1.wav\\
	| - audio2.wav\\
	| - audio3.wav\\
	...
\end{center}

Las transcripciones se recogen dentro del archivo metadata.csv. Donde audio1, audio2, etc se refieren a los archivos audio1.wav, audio2.wav etc.

\begin{center}
	audio1|Esta es mi transcripción 1.
	
	audio2|Esta es mi transcripción 2.
	
	audio3|Esta es mi transcripción 3.
	
	audio4|Esta es mi transcripción 4.
\end{center}

El modelo sobre el que se realiza el ajuste, está preentrenado sobre la base de datos en Español de \textit{The M-AiLabs Speech Dataset}, por tanto utilizaremos la misma estructura de este en la conformación de la base de datos con voces cubanas. Finalmente quedando la siguiente estructura:

\begin{flushleft}
	MyDataset/by$\_$book/female/[creador del dataset]/[nombre del hablante]
	
	|/wavs
	
	|metadata.csv
\end{flushleft}






\subsection{Procesamiento de audio}

\textbf{RNNoise} es una biblioteca basada en una red neuronal para la eliminación de ruido en grabaciones, se utliza en este proyecto para obtener clips de audio libres de ruidos y con la frecuencia de muestreo deseada.

Se realizó un procesamiento para la eliminación de ruido en el audio del \textit{dataset} original, y se estableció una frecuencia de muestreo de acuerdo a las necesidades de cada experimento. 

\section{Herramientas}
\subsection{Colab}
\subsection{Google Drive}
\subsection{Phonemizer espeak}

\section{Modificación en el código fuente de Coqui TTS}
(lo de formatter.py)

\section{Fine-Tuning de Tacontron-DDC}
(Estas son ideas sueltas, sin redactar)

-se utilizó la configuración de el modelo preentrenado
se desacargó el modelo Tacotron, y se puso a entrenar usando cuda(decir q es) en colab, y el script de configuración.

-requiere mucha ram, con 25gb no era suficiente, con 83 sí, no se la cantidad exacta requerida.

-sample rate 16000 pq es sobre el que está entrenado el tacotron en espannol de mailabs.

-Demora mucho el entrenamiento y toma muchos recursos.

-Con 160 epochs se escucha una grabación con mucho ruido, sin embargo en algún momento se puede distinguir alguna sílaba.

-Con 210 epochs algo pasa que las salidad para una oración son audios de 2 minutos donde solo se distingue ruido, y a partir de ahí para 451 y 580 epochs, este comportamiento se mantienen invariante. Se tenía previsto alcanzar las 2000 epochs, pero como el modelo consumía muchos recursos y los resultados no eran los deseados, no se continuó.

-Los resultados no podían estar más lejos de lo esperado, primero se pensaba que iba a ser un reentrenamiento veloz e iba a converger rápidamente en el nuevo dataset 

\section{Entrenamiento de modelo VITS desde cero}
Como los resultados de Tacotron2 no fueron los mejores, se optó por realizar experimentos con otros modelos, se eligió VITS por ser de los que mejores resultados arroja por encima de Tacotron2 y Glow-TTS[\cite{kim2021conditional}.]

Esta vez se entrena el modelo desde cero utilizando el conjunto de datos con voces cubanas, y la receta que provee la biblioteca[\cite{coqui-doc}] para entrenar VITS[\cite{train-vits}]

\section{Fine-tuning de modelos VITS preentrenados}